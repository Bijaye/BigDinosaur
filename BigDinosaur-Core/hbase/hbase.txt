pseudo-distributed mode:

In local file system home/testuser/hbase
HBase and ZooKeeper write data

Apache HBase is used to have random, real-time read/write access to Big Data.

It hosts very large tables on top of clusters of commodity hardware.

Apache HBase is a non-relational database modeled after Google's Bigtable. Bigtable acts up on Google File System, likewise Apache HBase works on top of Hadoop and HDFS.

It is used whenever there is a need to write heavy applications.
HBase is used whenever we need to provide fast random access to available data.

---------------Randow access --------------
Random real time crud operation 
distributed designed to server millions of rows and columns 
indexing is done concpet is simple 

HBase stores data in HFiles that are indexed (sorted) by their key. Given a random key, the client can determine when region server to ask for the row from. The region server can determine which region to retrieve the row from, and then do a binary search through the region to access the correct row. This is accomplished by having sufficient statistics to know the number of blocks, block size, start key, and end key.


concept 
Simple concept is used.Concept of region.Data is divided into different regions .
If data is divided into regions then it will help for fast data recovery,easy to load balance 
can be moved on another server based on region 


HDFS files are write once files. You can append to files in some of the recent versions but that is not a feature that is very commonly used. Consider HDFS files as write-once and read-many files. There is no concept of random writes.
3. HDFS doesn't do random reads very well.



-------it is actually optimized for streaming access of large files


HBase on the other hand is a database that stores it's data in a distributed filesystem. The filesystem of choice typically is HDFS owing to the tight integration between HBase and HDFS. 
HBase provides you with the following:
1. Low latency access to small amounts of data from within a large data set. You can access single rows quickly from a billion row table.
2. Flexible data model to work with and data is indexed by the row key.
3. Fast scans across tables.
4. Scale in terms of writes as well as total volume of data.
